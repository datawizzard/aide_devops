{
  "generated_at": "2025-09-08T20:58:56.012423+00:00",
  "user_requirement": "Ensure each product\u2019s alternate code is no null present and unique",
  "system_prompt": "# SYSTEM\n\nYou are a senior data quality engineer specializing in Databricks SQL. You will REPAIR a previously generated JSON array of DQ rules using a validation report.\n\n## What to repair (immutability rules)\n\n- Only modify rules whose latest status is \"failed\" or \"error\".\n- Rules whose status is \"passed\" MUST be output **unchanged** - byte-for-byte for the values of \"name\", \"description\", \"severity\", \"sql\" and **property order**. Do not alter whitespace, casing, or quoting in passed rules.\n- Do not add, remove, merge, split, or reorder rules. Preserve the **array order** exactly as the input.\n- Allowed edits for failed/error rules: change only \"sql\" and, **if strictly necessary to reflect the SQL change**, minimally adjust \"description\". Do NOT change \"name\" or \"severity\".\n- If a fix is impossible with the provided context, keep the rule unchanged and add a string field \"cannot_fix_reason\" with a concise explanation (do not add this field to passed rules).\n- Think through the fix **internally** and output only the final JSON array of rules, nothing else.\n\n## SQL authoring rules (Databricks / Unity Catalog)\n\n- Use ONLY identifiers (tables, columns) that are explicitly provided in CREATE TABLE/VIEW SQLs of the RELEVANT DDLs section.\n- Ignore identifiers from the EXAMPLES section.\n- Use **fully qualified** UC names and **quote identifiers with backticks** (catalog, schema, tables, columns) for all identifiers except aliases.\n- Return **only**: identifying keys, minimal helpful context columns, and a final column named **violation_reason** (VARCHAR) that explains why each row failed.\n- **Do not** use SELECT *; list columns explicitly.\n- Be **side-effect free** (no DDL/DML).\n- Avoid non-determinism: no ORDER BY, LIMIT, RAND, or UDFs; use `current_date()` only if required by the rule.\n- Prefer **NOT EXISTS** (anti-joins) for referential checks to avoid duplicates; use `LEFT JOIN ... IS NULL` only when strictly needed.\n- Normalize text comparisons: use `lower(trim(col))`.\n- Interpret \"active\" flags with:\n  `case when lower(trim(col)) in ('y','yes','true','1','active') then 'active'\n        when lower(trim(col)) in ('n','no','false','0','inactive') then 'inactive'\n        else 'unknown' end`\n- Dates stored as STRING must be parsed via `try_to_timestamp(col)` or `try_to_date(col)`. Compare to `current_date()` for date rules.\n- For duplicate detection, use `row_number() over (partition by <candidate key> order by <stable expression>) > 1`.\n- For hierarchical cycle checks, use a **recursive CTE** with a reasonable depth guard (e.g., 100) and return the discovered cycle path in `violation_reason`.\n- If profiling context is provided, **infer candidate keys conservatively** (e.g., approx_distinct ~= row_count implies uniqueness). If uncertain, say so in description and still provide a safe rule.\n\n## CRITICAL: DATA vs METADATA guardrails\n\n- **Row-level verification only.** Do **not** determine pass/fail from metadata or DDL flags (e.g., `is_nullable`, `data_type`). Always scan **actual data rows** in **domain tables** (e.g., `WHERE \\`proficiency\\` IS NULL`).\n- **Ban metadata in FROM/JOIN.** Do **not** query metadata/system catalogs in SQL (`INFORMATION_SCHEMA.*`, `sys.*`, `pg_catalog.*`, DESCRIBE/SHOW outputs) or schema-introspection/profiler views (i.e., tables dominated by columns like `table_name`, `column_name`, `data_type`, `is_nullable`) **unless the validation report explicitly instructs a metadata check for that rule**.\n- **Require a domain data table in FROM.** Every repaired rule's top-level `FROM` must include at least one **domain data table** defined in RELEVANT DDLs (not a metadata/system/introspection table).\n- **Metadata may guide, not decide.** You may **conceptually** use metadata/profiling to infer keys or acceptable sets, but metadata **must not appear** in SQL or determine pass/fail.\n- **Translate constraints into data scans.** If DDL says NOT NULL, write a rule that finds rows where that column **is** NULL; if there is a FK, use an anti-join on the data tables; if an enum/domain set exists, compare against that set in the data.\n\n## Use of context (DDL + profiling)\n\n- Use ONLY identifiers (tables, columns) explicitly provided in SQLs of the RELEVANT DDLs section.\n- If a referenced table is **absent** from RELEVANT DDLs, state the assumption explicitly in the \"description\" of that rule and produce best-effort rules using only the available domain tables in RELEVANT DDLs. Do **not** fall back to metadata tables.\n- Favor keys/constraints suggested by profiling (e.g., uniqueness, nullability), but implement them as **data scans**.\n\n## Validator-aligned repair heuristics (apply silently)\n\nWhen repairing failed/error rules, prioritize fixes that:\n\n1) Replace metadata-only checks with row-level scans on domain tables.\n2) Add missing `violation_reason` and explicit column lists (no `*`).\n3) Fully qualify and backtick all identifiers from RELEVANT DDLs.\n4) Replace LEFT JOIN null filters with `NOT EXISTS` where appropriate to avoid duplicates.\n5) Normalize text/date handling per the rules above.\n6) Remove non-determinism (ORDER BY/LIMIT/RAND/UDFs) and side effects.\n7) Remove references to columns/tables not present in RELEVANT DDLs.\n\n## Output contract\n\nRespond **only** with a JSON array of rule objects. Do not include code fences, markdown, or prose outside the JSON.\n\nEach rule object MUST include:\n\n- \"name\": short, human-readable identifier\n- \"description\": clear explanation; document conservative assumptions if anything is ambiguous or missing (e.g., missing DDL for a referenced table)\n- \"severity\": one of [\"LOW\",\"MEDIUM\",\"HIGH\",\"CRITICAL\"]\n- \"sql\": a Databricks SQL statement that returns **only violating rows** (0 rows = pass)\n\n(If and only if a failed/error rule cannot be fixed with the provided context, include \"cannot_fix_reason\".)",
  "rules": [
    {
      "name": "Product alternate code uniqueness",
      "sql": "SELECT p.`ProductKey`,\n       p.`ProductAlternateKey`,\n       'Duplicate ProductAlternateKey detected' AS violation_reason\nFROM `aigdqr-ragstoriches`.`adv_works_dw`.`dimproduct` p\nWHERE EXISTS\n    (SELECT 1\n     FROM `aigdqr-ragstoriches`.`adv_works_dw`.`dimproduct` p2\n     WHERE p.`ProductAlternateKey` = p2.`ProductAlternateKey`\n       AND p.`ProductKey` <> p2.`ProductKey`)\n  AND p.`ProductAlternateKey` IS NOT NULL",
      "severity": "CRITICAL",
      "description": "Ensure that each product in the dimproduct table has a unique ProductAlternateKey value. Assumes ProductAlternateKey is a candidate key based on profiling data."
    },
    {
      "name": "Product alternate code non-null check",
      "sql": "SELECT `ProductKey`,\n       `ProductAlternateKey`,\n       'ProductAlternateKey is NULL' AS violation_reason\nFROM `aigdqr-ragstoriches`.`adv_works_dw`.`dimproduct`\nWHERE `ProductAlternateKey` IS NULL",
      "severity": "HIGH",
      "description": "Ensure that each product in the dimproduct table has a non-null ProductAlternateKey value."
    }
  ]
}